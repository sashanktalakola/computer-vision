{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ae8878a-4315-4c53-b69d-8ba217631e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f01d46-9a81-46de-ad4e-ffeea0f26be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07661a1b-257c-4f3c-9c20-0736923bfba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, image_size=224, patch_size=16, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_paths (list of str): List of file paths to the images.\n",
    "            labels (list of int): List of labels corresponding to each image.\n",
    "            image_size (int): Size to which images should be resized (e.g., 224x224).\n",
    "            patch_size (int): Size of each patch (e.g., 16x16).\n",
    "            transform (callable, optional): Optional transform to be applied to each sample.\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of images in the dataset.\"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def _image_to_patches(self, image):\n",
    "        \"\"\"\n",
    "        Convert image into patches.\n",
    "        \n",
    "        Args:\n",
    "            image (PIL.Image or Tensor): The input image.\n",
    "        \n",
    "        Returns:\n",
    "            patches (Tensor): A tensor of patches shaped (num_patches, patch_dim).\n",
    "        \"\"\"\n",
    "        # Resize image to the required input size for ViT (224x224 by default)\n",
    "        image = image.resize((self.image_size, self.image_size))\n",
    "        \n",
    "        # Convert to tensor\n",
    "        image = transforms.ToTensor()(image)\n",
    "        \n",
    "        # Divide the image into patches\n",
    "        patches = []\n",
    "        for i in range(0, self.image_size, self.patch_size):\n",
    "            for j in range(0, self.image_size, self.patch_size):\n",
    "                patch = image[:, i:i+self.patch_size, j:j+self.patch_size]  # Extract a patch\n",
    "                patches.append(patch.flatten())  # Flatten each patch into a vector\n",
    "        \n",
    "        # Stack patches to get a tensor of shape (num_patches, patch_dim)\n",
    "        return torch.stack(patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Load and return a sample from the dataset at the given index.\"\"\"\n",
    "        # Load the image\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        \n",
    "        # Convert image to patches\n",
    "        patches = self._image_to_patches(image)\n",
    "        \n",
    "        # Get the label for this image\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # If a transform is specified, apply it\n",
    "        if self.transform:\n",
    "            patches = self.transform(patches)\n",
    "        \n",
    "        return patches, label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a1f0a-167c-49a3-ac70-32c7e6555045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "570ba3a9-2316-4f5d-a857-e3eda9131655",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchPositionalEmbeddingWithCLS(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=16, num_channels=3, embedding_dim=768):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_size (int): The size of the input image (e.g., 224x224).\n",
    "            patch_size (int): The size of each patch (e.g., 16x16).\n",
    "            num_channels (int): Number of input channels (3 for RGB).\n",
    "            embedding_dim (int): The size of the embedding (transformer token dimension).\n",
    "        \"\"\"\n",
    "        super(PatchPositionalEmbeddingWithCLS, self).__init__()\n",
    "\n",
    "        # Image size and patch size parameters\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Compute the number of patches in the image\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        \n",
    "        # The patch size is (patch_size, patch_size) in height/width, so the flattened patch dimension\n",
    "        self.patch_dim = patch_size * patch_size * num_channels\n",
    "        \n",
    "        # Linear projection layer to embed patches\n",
    "        self.patch_projection = nn.Linear(self.patch_dim, self.embedding_dim)\n",
    "\n",
    "        # Learnable positional embeddings (shape: [1, num_patches, embedding_dim])\n",
    "        self.positional_embeddings = nn.Parameter(torch.randn(1, self.num_patches, self.embedding_dim))\n",
    "\n",
    "        # Learnable class token ([CLS] token)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.embedding_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "        1. Project patches to embedding space.\n",
    "        2. Add positional embeddings.\n",
    "        3. Prepend the [CLS] token to the patch embeddings.\n",
    "        \"\"\"\n",
    "        # Step 1: Project patches to embedding space\n",
    "        patch_embeddings = self.patch_projection(x)\n",
    "        \n",
    "        # Step 2: Add positional embeddings (broadcasted over batch)\n",
    "        patch_embeddings_with_pos = patch_embeddings + self.positional_embeddings\n",
    "        \n",
    "        # Step 3: Prepend the [CLS] token to the patch embeddings\n",
    "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1)  # Expand the [CLS] token to the batch size\n",
    "        patch_embeddings_with_cls = torch.cat((cls_tokens, patch_embeddings_with_pos), dim=1)\n",
    "        \n",
    "        return patch_embeddings_with_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d2648b-a86c-4cf3-8edc-fece2355a860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8d6ccf7-e6df-486f-838e-f95d56ed41d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim=768, num_heads=12, ff_hidden_dim=3072, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_dim (int): The dimension of the patch embeddings (e.g., 768).\n",
    "            num_heads (int): The number of attention heads for multi-head attention.\n",
    "            ff_hidden_dim (int): The hidden dimension of the feedforward network (usually larger than embedding_dim).\n",
    "            dropout (float): Dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        # Multi-Head Self Attention (MSA)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout)\n",
    "\n",
    "        # Feedforward Network (FFN)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, ff_hidden_dim),  # First linear layer\n",
    "            nn.GELU(),                               # Activation\n",
    "            nn.Dropout(dropout),                     # Dropout\n",
    "            nn.Linear(ff_hidden_dim, embedding_dim)  # Second linear layer\n",
    "        )\n",
    "\n",
    "        # LayerNorm layers (pre-LayerNorm)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # Dropout layers for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder block:\n",
    "        1. Apply LayerNorm before Multi-Head Self Attention (MSA).\n",
    "        2. Multi-Head Self Attention (MSA).\n",
    "        3. Add residual connections and apply LayerNorm.\n",
    "        4. Apply LayerNorm before Feedforward Network (FFN).\n",
    "        5. Feedforward Network (FFN).\n",
    "        6. Add residual connections and apply LayerNorm.\n",
    "        \"\"\"\n",
    "        # Step 1: Apply LayerNorm before MSA\n",
    "        x_norm = self.norm1(x)  # Normalize input to attention layer\n",
    "        attn_output, _ = self.attention(x_norm, x_norm, x_norm)  # Self-attention\n",
    "        \n",
    "        # Step 2: Add residual connection and apply LayerNorm\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x_norm = self.norm2(x)  # Normalize input to FFN layer\n",
    "        \n",
    "        # Step 3: Feedforward Network (FFN)\n",
    "        ffn_output = self.ffn(x_norm)\n",
    "        \n",
    "        # Step 4: Add residual connection and apply LayerNorm\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340d9fe-31a8-4c71-b1a6-573f2f74e53c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f163b265-049d-48b6-ae05-ba7b80a8036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, embedding_dim=768, num_classes=1000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_dim (int): The dimension of the patch embeddings (e.g., 768).\n",
    "            num_classes (int): The number of classes for classification.\n",
    "        \"\"\"\n",
    "        super(MLPHead, self).__init__()\n",
    "\n",
    "        # First fully connected layer (from embedding dimension to 2048)\n",
    "        self.fc1 = nn.Linear(embedding_dim, 2048)\n",
    "\n",
    "        # Second fully connected layer (from 2048 to the number of classes)\n",
    "        self.fc2 = nn.Linear(2048, num_classes)\n",
    "\n",
    "        # GELU activation function after the first fully connected layer\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, cls_token_output):\n",
    "        \"\"\"\n",
    "        Forward pass through the MLP head:\n",
    "        1. Apply the first fully connected layer and GELU activation.\n",
    "        2. Apply the second fully connected layer to produce the final logits.\n",
    "        \"\"\"\n",
    "        # Step 1: Apply the first fully connected layer followed by GELU activation\n",
    "        x = self.gelu(self.fc1(cls_token_output))  # Shape: [batch_size, 2048]\n",
    "\n",
    "        # Step 2: Apply the second fully connected layer (output layer) to produce logits\n",
    "        x = self.fc2(x)  # Shape: [batch_size, num_classes]\n",
    "\n",
    "        # Return the logits (raw class scores before applying softmax)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8fc1d6-42e5-4331-bb71-e56c152bacec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f43b077-e4a5-41c7-a8c2-4686af435e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTModel(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=16, embedding_dim=768, num_heads=12, ff_hidden_dim=3072, num_classes=1000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_size (int): The size of the input image (e.g., 224x224).\n",
    "            patch_size (int): The size of each patch (e.g., 16x16).\n",
    "            embedding_dim (int): The dimension of the patch embeddings (e.g., 768).\n",
    "            num_heads (int): The number of attention heads for multi-head attention.\n",
    "            ff_hidden_dim (int): The hidden dimension of the feedforward network.\n",
    "            num_classes (int): The number of output classes for classification.\n",
    "            dropout (float): Dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super(ViTModel, self).__init__()\n",
    "\n",
    "        # Patch + Positional Embedding with [CLS] token\n",
    "        self.patch_positional_embedding = PatchPositionalEmbeddingWithCLS(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "\n",
    "        # Encoder Block: This processes the image patches including the [CLS] token\n",
    "        self.encoder_block = EncoderBlock(\n",
    "            embedding_dim=embedding_dim, \n",
    "            num_heads=num_heads, \n",
    "            ff_hidden_dim=ff_hidden_dim, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # MLP Head: Classification head that uses the [CLS] token's output\n",
    "        self.mlp_head = MLPHead(embedding_dim=embedding_dim, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the model:\n",
    "        1. Input tensor x (batch_size, num_channels, image_size, image_size).\n",
    "        2. Pass through PatchPositionalEmbeddingWithCLS (to create embeddings and add [CLS] token).\n",
    "        3. Pass through encoder block (self-attention).\n",
    "        4. Extract the output of the [CLS] token (first token in the sequence).\n",
    "        5. Pass the [CLS] token's output to the MLP head for classification.\n",
    "        \"\"\"\n",
    "        # Step 1: Pass the input image through the patch embedding and positional encoding layer\n",
    "        x = self.patch_positional_embedding(x)  # Output shape: [batch_size, num_patches+1, embedding_dim]\n",
    "        \n",
    "        # Step 2: Pass the encoded patches through the encoder block (self-attention and feedforward)\n",
    "        encoder_output = self.encoder_block(x)  # Output shape: [batch_size, num_patches+1, embedding_dim]\n",
    "        \n",
    "        # Step 3: Extract the [CLS] token's output (first token in the sequence)\n",
    "        cls_token_output = encoder_output[:, 0, :]  # Shape: [batch_size, embedding_dim]\n",
    "        \n",
    "        # Step 4: Pass the [CLS] token's output through the MLP head for classification\n",
    "        logits = self.mlp_head(cls_token_output)  # Output shape: [batch_size, num_classes]\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca95b0e-6c58-4b4d-a865-0d1d3dec3596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
