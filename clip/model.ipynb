{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe2f102-0ffa-4eef-bd29-38b6248499e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTConfig, ViTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d62329-0d33-4104-9aa2-1f08d9ed5ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a72aa8-515b-4a3c-9dc6-77ac70d94ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPImageEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=224,\n",
    "        patch_size=32,\n",
    "        hidden_size=512,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=8,\n",
    "        projection_dim=512\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Configure ViT\n",
    "        self.config = ViTConfig(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=num_hidden_layers,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            num_channels=3,\n",
    "            qkv_bias=True,\n",
    "            layer_norm_eps=1e-6\n",
    "        )\n",
    "        \n",
    "        # Initialize ViT backbone\n",
    "        self.vit = ViTModel(self.config)\n",
    "        \n",
    "        # Projection layer\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # Get ViT outputs\n",
    "        outputs = self.vit(pixel_values)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Project to final dimension\n",
    "        projected = self.projection(pooled_output)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        image_features = projected / projected.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a4eedb-dcbe-4ec8-acd4-90e60e8e3ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = CLIPImageEncoder()\n",
    "\n",
    "batch_size = 32\n",
    "dummy_input = torch.randn(batch_size, 3, 224, 224)\n",
    "\n",
    "output = test_model(dummy_input)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6f15e-36b3-4ac0-b297-f2977797575c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
