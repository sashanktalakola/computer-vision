{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe2f102-0ffa-4eef-bd29-38b6248499e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from encoders import CLIPResNetEncoder, CLIPViTEncoder\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d62329-0d33-4104-9aa2-1f08d9ed5ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d6ffd-ce81-47af-a910-e4f9a6f29930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPGPT2Encoder(nn.Module):\n",
    "    def __init__(self, projection_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained('gpt2')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.LayerNorm(768),\n",
    "            nn.Linear(768, projection_dim)\n",
    "        )\n",
    "        \n",
    "        self.max_length = 77\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Mean pooling\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        pooled_output = (last_hidden_state * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1).unsqueeze(-1)\n",
    "        \n",
    "        projected = self.projection(pooled_output)\n",
    "        text_features = projected / projected.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f738e-f098-4025-a7c2-82af756355f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7cd779-348f-4034-9c98-fb9cc5f7d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, image_encoder, text_encoder, temperature=0.07):\n",
    "        super(CLIPModel, self).__init__()\n",
    "        \n",
    "        # Assuming projection heads are already incorporated in encoders\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        \n",
    "        # Temperature scaling constant (typically a small value, 0.07 in CLIP)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        # Encode the images and texts\n",
    "        image_features = self.image_encoder(images)  # Shape: (batch_size, feature_dim)\n",
    "        text_features = self.text_encoder(texts)    # Shape: (batch_size, feature_dim)\n",
    "        \n",
    "        # Normalize both features for cosine similarity calculation\n",
    "        image_features = F.normalize(image_features, p=2, dim=-1)\n",
    "        text_features = F.normalize(text_features, p=2, dim=-1)\n",
    "        \n",
    "        return image_features, text_features\n",
    "\n",
    "    def compute_loss(self, image_features, text_features):\n",
    "        # Cosine similarity between image and text features\n",
    "        logits_per_image = image_features @ text_features.T  # Shape: (batch_size, batch_size)\n",
    "        logits_per_text = text_features @ image_features.T  # Shape: (batch_size, batch_size)\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        logits_per_image /= self.temperature\n",
    "        logits_per_text /= self.temperature\n",
    "        \n",
    "        # Labels are the diagonal (the matching image-text pairs)\n",
    "        labels = torch.arange(image_features.size(0)).to(image_features.device)\n",
    "        \n",
    "        # Cross-entropy loss (image -> text and text -> image)\n",
    "        loss_img2txt = F.cross_entropy(logits_per_image, labels)\n",
    "        loss_txt2img = F.cross_entropy(logits_per_text, labels)\n",
    "        \n",
    "        # Total loss is the average of both directions\n",
    "        total_loss = (loss_img2txt + loss_txt2img) / 2\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b633e-8fb1-442d-96bc-ac44f799d3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
