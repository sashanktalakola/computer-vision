{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920c829-eee0-4b11-8cdd-40b3b0ac3505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad30b2-ce91-48d5-8434-633a42e6fa5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82873b9-7007-416b-897a-2731ebf919ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, backbone_name='resnet50', pretrained=True, hidden_dim=256):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "\n",
    "        backbone = models.resnet50(pretrained=pretrained)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.conv1x1 = nn.Conv2d(backbone.fc.in_features, hidden_dim, kernel_size=1)\n",
    "        \n",
    "        self.positional_encoding = self._get_positional_encoding()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = self.conv1x1(features)\n",
    "        \n",
    "        features = features + self.positional_encoding\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def _get_positional_encoding(self, height=32, width=32):\n",
    "        pe = torch.zeros(self.hidden_dim, height, width)\n",
    "        y, x = torch.meshgrid(torch.arange(height), torch.arange(width), indexing='ij')\n",
    "        div_term = torch.exp(torch.arange(0., self.hidden_dim, 2) * -(torch.log(torch.tensor(10000.0)) / self.hidden_dim))\n",
    "        \n",
    "        pe[0::2, :, :] = torch.sin(x.unsqueeze(0) * div_term.unsqueeze(1).unsqueeze(2))\n",
    "        pe[1::2, :, :] = torch.cos(x.unsqueeze(0) * div_term.unsqueeze(1).unsqueeze(2))\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b648a58-4484-40f3-8bf0-e03314a56b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801d91a-a51f-4e58-9537-b8c6d13b5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, nhead=8, dim_feedforward=2048):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=nhead)\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=nhead)\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_dim, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, hidden_dim)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        \n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)[0]\n",
    "        tgt = self.norm1(tgt + tgt2)\n",
    "        \n",
    "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask)[0]\n",
    "        tgt = self.norm2(tgt + tgt2)\n",
    "        \n",
    "        tgt2 = self.linear2(F.relu(self.linear1(tgt)))\n",
    "        tgt = self.norm3(tgt + tgt2)\n",
    "        \n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe556c-8d45-49f1-b8db-76a5635460a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, num_queries=100, num_classes=91, nhead=8, num_layers=6, dim_feedforward=2048):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.num_queries = num_queries\n",
    "        self.detr_output_dim = hidden_dim\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dim_feedforward=dim_feedforward),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.decoder_layer = TransformerDecoderLayer(hidden_dim, nhead, dim_feedforward)\n",
    "        \n",
    "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
    "\n",
    "        self.class_embed = nn.Linear(hidden_dim, num_classes)\n",
    "        self.bbox_embed = nn.Linear(hidden_dim, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, height, width = x.shape\n",
    "        memory = self.encoder(x.flatten(2).transpose(1, 2))\n",
    "\n",
    "        queries = self.query_embed.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "        output = self.decoder_layer(queries, memory)\n",
    "\n",
    "        class_logits = self.class_embed(output)\n",
    "        bbox_preds = self.bbox_embed(output)\n",
    "\n",
    "        return class_logits, bbox_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388cd1e8-1ac6-4d26-9eba-bed58532e1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e6474-6d0e-4f56-a57a-435aa0beca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETR(nn.Module):\n",
    "    def __init__(self, num_queries=100, num_classes=91, hidden_dim=256, nhead=8, num_layers=6, dim_feedforward=2048):\n",
    "        super(DETR, self).__init__()\n",
    "\n",
    "        self.backbone = ImageEncoder(hidden_dim=hidden_dim)\n",
    "        self.transformer = TransformerModel(\n",
    "            hidden_dim=hidden_dim, \n",
    "            num_queries=num_queries, \n",
    "            num_classes=num_classes,\n",
    "            nhead=nhead, \n",
    "            num_layers=num_layers, \n",
    "            dim_feedforward=dim_feedforward\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        class_logits, bbox_preds = self.transformer(features)\n",
    "        return class_logits, bbox_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ce15f-f6d9-4866-a89f-fad39ad22e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e35d34-6cce-42f7-a5c5-be353d716ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetCriterion(nn.Module):\n",
    "    def __init__(self, num_classes, matcher=None, weight_dict=None, eos_coef=0.1):\n",
    "        super(SetCriterion, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.matcher = matcher or GreedyMatcher()\n",
    "        self.weight_dict = weight_dict or {\n",
    "            'loss_ce': 1,\n",
    "            'loss_bbox': 5,\n",
    "            'loss_giou': 2\n",
    "        }\n",
    "        self.eos_coef = eos_coef\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        class_logits = outputs['class_logits']\n",
    "        bbox_preds = outputs['bbox_preds']\n",
    "\n",
    "        matched_indices = self.matcher(class_logits, bbox_preds, targets)\n",
    "\n",
    "        losses = self.compute_loss(class_logits, bbox_preds, targets, matched_indices)\n",
    "        return losses\n",
    "    \n",
    "    def compute_loss(self, class_logits, bbox_preds, targets, matched_indices):\n",
    "        losses = {}\n",
    "        \n",
    "        class_loss = self.compute_class_loss(class_logits, targets, matched_indices)\n",
    "        losses['loss_ce'] = class_loss\n",
    "        \n",
    "        bbox_loss = self.compute_bbox_loss(bbox_preds, targets, matched_indices)\n",
    "        losses['loss_bbox'] = bbox_loss\n",
    "        \n",
    "        giou_loss = self.compute_giou_loss(bbox_preds, targets, matched_indices)\n",
    "        losses['loss_giou'] = giou_loss\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def compute_class_loss(self, class_logits, targets, matched_indices):\n",
    "        target_classes_o = torch.cat([t['labels'] for t in targets], dim=0)\n",
    "        target_classes_o = target_classes_o[matched_indices[1]]\n",
    "\n",
    "        loss_ce = F.cross_entropy(class_logits.view(-1, self.num_classes), target_classes_o, reduction='mean')\n",
    "        return loss_ce\n",
    "\n",
    "    def compute_bbox_loss(self, bbox_preds, targets, matched_indices):\n",
    "        target_boxes = torch.cat([t['boxes'] for t in targets], dim=0)\n",
    "        target_boxes = target_boxes[matched_indices[1]]\n",
    "\n",
    "        loss_bbox = F.l1_loss(bbox_preds.view(-1, 4), target_boxes, reduction='mean')\n",
    "        return loss_bbox\n",
    "\n",
    "    def compute_giou_loss(self, bbox_preds, targets, matched_indices):\n",
    "        target_boxes = torch.cat([t['boxes'] for t in targets], dim=0)\n",
    "        target_boxes = target_boxes[matched_indices[1]]\n",
    "\n",
    "        giou_loss = generalized_iou_loss(bbox_preds.view(-1, 4), target_boxes)\n",
    "        return giou_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303e998d-d968-4837-af69-deac5006cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_iou_loss(pred_boxes, target_boxes):\n",
    "    pred_x1, pred_y1, pred_x2, pred_y2 = pred_boxes.unbind(1)\n",
    "    target_x1, target_y1, target_x2, target_y2 = target_boxes.unbind(1)\n",
    "\n",
    "    pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)\n",
    "    target_area = (target_x2 - target_x1) * (target_y2 - target_y1)\n",
    "\n",
    "    inter_x1 = torch.max(pred_x1, target_x1)\n",
    "    inter_y1 = torch.max(pred_y1, target_y1)\n",
    "    inter_x2 = torch.min(pred_x2, target_x2)\n",
    "    inter_y2 = torch.min(pred_y2, target_y2)\n",
    "\n",
    "    inter_area = (inter_x2 - inter_x1).clamp(min=0) * (inter_y2 - inter_y1).clamp(min=0)\n",
    "\n",
    "    union_area = pred_area + target_area - inter_area\n",
    "    iou = inter_area / union_area\n",
    "\n",
    "    convex_area = (torch.max(pred_x2, target_x2) - torch.min(pred_x1, target_x1)) * \\\n",
    "                  (torch.max(pred_y2, target_y2) - torch.min(pred_y1, target_y1))\n",
    "\n",
    "    giou = iou - (convex_area - union_area) / convex_area\n",
    "\n",
    "    return 1 - giou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c9e95-1f00-43cd-b2ba-87be52996771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909263ff-8524-4b79-9c85-aa5e68bdd1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
