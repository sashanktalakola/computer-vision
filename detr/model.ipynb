{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920c829-eee0-4b11-8cdd-40b3b0ac3505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad30b2-ce91-48d5-8434-633a42e6fa5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82873b9-7007-416b-897a-2731ebf919ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, backbone_name='resnet50', pretrained=True, hidden_dim=256):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "\n",
    "        backbone = models.resnet50(pretrained=pretrained)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.conv1x1 = nn.Conv2d(backbone.fc.in_features, hidden_dim, kernel_size=1)\n",
    "        \n",
    "        self.positional_encoding = self._get_positional_encoding()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = self.conv1x1(features)\n",
    "        \n",
    "        features = features + self.positional_encoding\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def _get_positional_encoding(self, height=32, width=32):\n",
    "        pe = torch.zeros(self.hidden_dim, height, width)\n",
    "        y, x = torch.meshgrid(torch.arange(height), torch.arange(width), indexing='ij')\n",
    "        div_term = torch.exp(torch.arange(0., self.hidden_dim, 2) * -(torch.log(torch.tensor(10000.0)) / self.hidden_dim))\n",
    "        \n",
    "        pe[0::2, :, :] = torch.sin(x.unsqueeze(0) * div_term.unsqueeze(1).unsqueeze(2))\n",
    "        pe[1::2, :, :] = torch.cos(x.unsqueeze(0) * div_term.unsqueeze(1).unsqueeze(2))\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b648a58-4484-40f3-8bf0-e03314a56b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801d91a-a51f-4e58-9537-b8c6d13b5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, nhead=8, dim_feedforward=2048):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=nhead)\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=nhead)\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_dim, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, hidden_dim)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        \n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)[0]\n",
    "        tgt = self.norm1(tgt + tgt2)\n",
    "        \n",
    "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask)[0]\n",
    "        tgt = self.norm2(tgt + tgt2)\n",
    "        \n",
    "        tgt2 = self.linear2(F.relu(self.linear1(tgt)))\n",
    "        tgt = self.norm3(tgt + tgt2)\n",
    "        \n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909263ff-8524-4b79-9c85-aa5e68bdd1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
