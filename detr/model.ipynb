{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920c829-eee0-4b11-8cdd-40b3b0ac3505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad30b2-ce91-48d5-8434-633a42e6fa5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82873b9-7007-416b-897a-2731ebf919ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, backbone_name='resnet50', pretrained=True, hidden_dim=256):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "\n",
    "        backbone = models.resnet50(pretrained=pretrained)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.conv1x1 = nn.Conv2d(backbone.fc.in_features, hidden_dim, kernel_size=1)\n",
    "        \n",
    "        self.positional_encoding = self._get_positional_encoding()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = self.conv1x1(features)\n",
    "        \n",
    "        features = features + self.positional_encoding\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def _get_positional_encoding(self, height=32, width=32):\n",
    "        pe = torch.zeros(self.hidden_dim, height, width)\n",
    "        y, x = torch.meshgrid(torch.arange(height), torch.arange(width), indexing='ij')\n",
    "        div_term = torch.exp(torch.arange(0., self.hidden_dim, 2) * -(torch.log(torch.tensor(10000.0)) / self.hidden_dim))\n",
    "        \n",
    "        pe[0::2, :, :] = torch.sin(x.unsqueeze(0) * div_term.unsqueeze(1).unsqueeze(2))\n",
    "        pe[1::2, :, :] = torch.cos(x.unsqueeze(0) * div_term.unsqueeze(1).unsqueeze(2))\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b648a58-4484-40f3-8bf0-e03314a56b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801d91a-a51f-4e58-9537-b8c6d13b5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, nhead=8, dim_feedforward=2048):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=nhead)\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=nhead)\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_dim, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, hidden_dim)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        \n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)[0]\n",
    "        tgt = self.norm1(tgt + tgt2)\n",
    "        \n",
    "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask)[0]\n",
    "        tgt = self.norm2(tgt + tgt2)\n",
    "        \n",
    "        tgt2 = self.linear2(F.relu(self.linear1(tgt)))\n",
    "        tgt = self.norm3(tgt + tgt2)\n",
    "        \n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe556c-8d45-49f1-b8db-76a5635460a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, num_queries=100, num_classes=91, nhead=8, num_layers=6, dim_feedforward=2048):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.num_queries = num_queries\n",
    "        self.detr_output_dim = hidden_dim\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dim_feedforward=dim_feedforward),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.decoder_layer = TransformerDecoderLayer(hidden_dim, nhead, dim_feedforward)\n",
    "        \n",
    "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
    "\n",
    "        self.class_embed = nn.Linear(hidden_dim, num_classes)\n",
    "        self.bbox_embed = nn.Linear(hidden_dim, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, height, width = x.shape\n",
    "        memory = self.encoder(x.flatten(2).transpose(1, 2))\n",
    "\n",
    "        queries = self.query_embed.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "        output = self.decoder_layer(queries, memory)\n",
    "\n",
    "        class_logits = self.class_embed(output)\n",
    "        bbox_preds = self.bbox_embed(output)\n",
    "\n",
    "        return class_logits, bbox_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388cd1e8-1ac6-4d26-9eba-bed58532e1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e6474-6d0e-4f56-a57a-435aa0beca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETR(nn.Module):\n",
    "    def __init__(self, num_queries=100, num_classes=91, hidden_dim=256, nhead=8, num_layers=6, dim_feedforward=2048):\n",
    "        super(DETR, self).__init__()\n",
    "\n",
    "        self.backbone = ImageEncoder(hidden_dim=hidden_dim)\n",
    "        self.transformer = TransformerModel(\n",
    "            hidden_dim=hidden_dim, \n",
    "            num_queries=num_queries, \n",
    "            num_classes=num_classes,\n",
    "            nhead=nhead, \n",
    "            num_layers=num_layers, \n",
    "            dim_feedforward=dim_feedforward\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        class_logits, bbox_preds = self.transformer(features)\n",
    "        return class_logits, bbox_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909263ff-8524-4b79-9c85-aa5e68bdd1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
