{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d1c4f-846d-49fa-9dea-d5cc7b335727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8992f384-2c36-45be-9a7d-afd1c15b6c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(Backbone, self).__init__()\n",
    "        \n",
    "        # Load a pretrained ResNet50 backbone from torchvision\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "        \n",
    "        # Use layers before the fully connected layer (resnet.fc)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the layers of the backbone, storing outputs at different stages\n",
    "        features = []\n",
    "        \n",
    "        # Pass through the layers of the backbone, storing outputs at different stages\n",
    "        for name, module in self.backbone.named_children():\n",
    "            x = module(x)\n",
    "            if name in ['4', '5', '6']:  # These are stages where features are produced\n",
    "                features.append(x)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd05339-ee06-4aee-be94-bf32c5d65301",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopDownPathway(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(TopDownPathway, self).__init__()\n",
    "        \n",
    "        # List of convolutional layers to reduce the number of channels after each upsampling\n",
    "        self.lateral_convs = nn.ModuleList([nn.Conv2d(in_ch, out_channels, kernel_size=1) \n",
    "                                            for in_ch in in_channels])\n",
    "        \n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        :param features: List of feature maps from the backbone network\n",
    "                         Each feature map in the list is of shape (batch_size, channels, height, width)\n",
    "        :return: List of feature maps after top-down aggregation.\n",
    "        \"\"\"\n",
    "        # Make sure the input list is ordered from shallowest to deepest\n",
    "        assert len(features) == len(self.lateral_convs)\n",
    "        \n",
    "        # Initialize the list to store the output of each stage\n",
    "        top_down_features = []\n",
    "        \n",
    "        # Start with the last (deepest) feature map\n",
    "        x = features[-1]\n",
    "        \n",
    "        # Iterate over the feature maps from deepest to shallowest\n",
    "        for i in range(len(features) - 2, -1, -1):\n",
    "            # Upsample the feature map\n",
    "            x = F.interpolate(x, size=features[i].shape[2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # Apply the lateral convolution to match the number of channels\n",
    "            lateral_x = self.lateral_convs[i](features[i])\n",
    "            \n",
    "            # Add the upsampled feature map to the current feature map\n",
    "            x = x + lateral_x\n",
    "            \n",
    "            # Append the processed feature map to the output list\n",
    "            top_down_features.append(x)\n",
    "        \n",
    "        # Reverse the list to maintain the order from shallowest to deepest\n",
    "        top_down_features.reverse()\n",
    "        \n",
    "        return top_down_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c3efd-a0d1-433e-8173-d458d5563376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9125185e-ed0b-46f2-8582-f76339925dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottomUpPathway(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        :param in_channels: List of channel sizes for each level of feature maps.\n",
    "        :param out_channels: The desired number of output channels for the aggregated features.\n",
    "        \"\"\"\n",
    "        super(BottomUpPathway, self).__init__()\n",
    "        \n",
    "        # Lateral convolutions to reduce the number of channels after each upsampling\n",
    "        self.lateral_convs = nn.ModuleList([nn.Conv2d(in_ch, out_channels, kernel_size=1) \n",
    "                                            for in_ch in in_channels])\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        :param features: List of feature maps (from shallow to deep) passed through the network.\n",
    "        :return: List of feature maps after bottom-up aggregation.\n",
    "        \"\"\"\n",
    "        # Ensure the input list is ordered from shallowest to deepest\n",
    "        assert len(features) == len(self.lateral_convs)\n",
    "        \n",
    "        # Initialize the list to store the output of each stage\n",
    "        bottom_up_features = []\n",
    "        \n",
    "        # Start with the shallowest feature map\n",
    "        x = features[0]\n",
    "        \n",
    "        # Iterate over the feature maps from shallow to deep\n",
    "        for i in range(1, len(features)):\n",
    "            # Upsample the feature map\n",
    "            x = F.interpolate(x, size=features[i].shape[2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # Apply the lateral convolution to match the number of channels\n",
    "            lateral_x = self.lateral_convs[i](features[i])\n",
    "            \n",
    "            # Add the upsampled feature map to the current feature map\n",
    "            x = x + lateral_x\n",
    "            \n",
    "            # Append the processed feature map to the output list\n",
    "            bottom_up_features.append(x)\n",
    "        \n",
    "        return bottom_up_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f810f5-f100-4136-aa6b-9664046820e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcf60ed-e777-4b3e-bb27-018bbed23f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveFeaturePooling(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        \"\"\"\n",
    "        :param output_size: The target output size (e.g., (1, 1) for global pooling or (7, 7) for a detailed representation)\n",
    "        \"\"\"\n",
    "        super(AdaptiveFeaturePooling, self).__init__()\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Input tensor (batch_size, channels, height, width)\n",
    "        :return: Pooled output of the target size\n",
    "        \"\"\"\n",
    "        # Apply adaptive average pooling\n",
    "        x = F.adaptive_avg_pool2d(x, self.output_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4f626c-8393-4771-8478-cd913ad88c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dead488-d9f9-4327-afae-a042c14f9a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxBranch(nn.Module):\n",
    "    def __init__(self, in_channels, num_anchors):\n",
    "        \"\"\"\n",
    "        :param in_channels: Number of input channels from the feature map.\n",
    "        :param num_anchors: Number of anchor boxes per location.\n",
    "        \"\"\"\n",
    "        super(BoxBranch, self).__init__()\n",
    "\n",
    "        # Convolutional layers to process the feature map before regressing bounding boxes\n",
    "        self.conv1 = nn.Conv2d(in_channels, 256, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Final layer to predict bounding box coordinates (dx, dy, dw, dh)\n",
    "        # For each anchor, we predict 4 values (dx, dy, dw, dh)\n",
    "        self.box_pred = nn.Conv2d(256, num_anchors * 4, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Feature map from the backbone or a subsequent layer (batch_size, channels, height, width)\n",
    "        :return: Predicted bounding box offsets (dx, dy, dw, dh) for each anchor.\n",
    "        \"\"\"\n",
    "        # Pass through convolutional layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        # Output bounding box predictions (dx, dy, dw, dh)\n",
    "        box_pred = self.box_pred(x)\n",
    "\n",
    "        # Reshape the predictions: (batch_size, num_anchors*4, height, width)\n",
    "        # num_anchors is the number of anchor boxes per spatial location\n",
    "        box_pred = box_pred.permute(0, 2, 3, 1).contiguous()\n",
    "        \n",
    "        # Reshape to (batch_size, height, width, num_anchors, 4)\n",
    "        # This makes it easier to match with the anchors for bounding box regression\n",
    "        box_pred = box_pred.view(x.size(0), x.size(2), x.size(3), -1, 4)\n",
    "        \n",
    "        return box_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf8475d-c453-4a02-bc17-f8eb899a41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassBranch(nn.Module):\n",
    "    def __init__(self, in_channels, num_anchors, num_classes):\n",
    "        \"\"\"\n",
    "        Classifies the object in each anchor box location.\n",
    "        \n",
    "        :param in_channels: The number of input channels from the feature map.\n",
    "                             Typically, this would be the number of channels from the output of \n",
    "                             the Top-Down or Bottom-Up Pathways.\n",
    "        :param num_anchors: The number of anchor boxes per spatial location in the feature map.\n",
    "                             For example, there might be 9 anchors for each grid location in the feature map.\n",
    "        :param num_classes: The number of object classes to predict, including the background class.\n",
    "                            This should be the number of classes in the dataset (e.g., 80 for COCO).\n",
    "        \"\"\"\n",
    "        super(ClassBranch, self).__init__()\n",
    "\n",
    "        # Convolutional layers to process the feature map before predicting class scores\n",
    "        # First convolution layer (with ReLU activation)\n",
    "        self.conv1 = nn.Conv2d(in_channels, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Second convolution layer (with ReLU activation)\n",
    "        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Final layer to predict class scores for each anchor box.\n",
    "        # For each anchor box, we predict `num_anchors * num_classes` values (e.g., 9 anchors * 80 classes = 720 output values).\n",
    "        self.class_pred = nn.Conv2d(256, num_anchors * num_classes, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the classification branch.\n",
    "        \n",
    "        :param x: The input feature map, which has the shape (batch_size, channels, height, width)\n",
    "                  and comes from either the Top-Down or Bottom-Up Pathway.\n",
    "        \n",
    "        :return: The predicted class scores for each anchor at every spatial location.\n",
    "                 The output tensor has the shape (batch_size, height, width, num_anchors, num_classes).\n",
    "                 This tensor contains the predicted probabilities for each anchor at each grid cell.\n",
    "        \"\"\"\n",
    "        # Apply the first convolution followed by ReLU activation\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        # Apply the second convolution followed by ReLU activation\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        # Predict the class scores for each anchor using the final convolution layer\n",
    "        class_pred = self.class_pred(x)\n",
    "        \n",
    "        # Permute the output to shape (batch_size, height, width, num_anchors * num_classes)\n",
    "        # This is necessary because we want the class predictions in the format (batch_size, H, W, num_anchors, num_classes)\n",
    "        class_pred = class_pred.permute(0, 2, 3, 1).contiguous()\n",
    "        \n",
    "        # Reshape the output to (batch_size, height, width, num_anchors, num_classes)\n",
    "        # This makes it easier to match the predictions with the anchors for classification.\n",
    "        class_pred = class_pred.view(x.size(0), x.size(2), x.size(3), -1, class_pred.size(1) // x.size(2) // x.size(3))\n",
    "        \n",
    "        return class_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775bedc6-377f-46b0-95e3-c3081e709959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef2700-96d9-421f-b1a0-931003087f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PANet(nn.Module):\n",
    "    def __init__(self, num_classes, num_anchors=9):\n",
    "        \"\"\"\n",
    "        PANet (Path Aggregation Network) for object detection.\n",
    "        \n",
    "        :param num_classes: The number of object classes (including the background class).\n",
    "        :param num_anchors: The number of anchor boxes per spatial location.\n",
    "        \"\"\"\n",
    "        super(PANet, self).__init__()\n",
    "        \n",
    "        # Backbone: Feature extraction (e.g., ResNet50)\n",
    "        self.backbone = Backbone(pretrained=True)\n",
    "        \n",
    "        # Top-Down Pathway (TDP)\n",
    "        self.top_down = TopDownPathway([1024, 512, 256], 256)  # Example input channels sizes\n",
    "        \n",
    "        # Bottom-Up Pathway (BUP)\n",
    "        self.bottom_up = BottomUpPathway([256, 512, 1024], 256)  # Example input channels sizes\n",
    "        \n",
    "        # Adaptive Feature Pooling to ensure fixed output sizes\n",
    "        self.adaptive_pool = AdaptiveFeaturePooling(output_size=(1, 1))\n",
    "        \n",
    "        # Box Branch: Predict bounding box offsets\n",
    "        self.box_branch = BoxBranch(256, num_anchors)\n",
    "        \n",
    "        # Class Branch: Predict object class scores\n",
    "        self.class_branch = ClassBranch(256, num_anchors, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the PANet network.\n",
    "        \n",
    "        :param x: The input tensor (batch_size, 3, height, width) representing the input image.\n",
    "        :return: A tuple containing:\n",
    "                 - `box_preds`: The predicted bounding box coordinates (dx, dy, dw, dh) for each anchor.\n",
    "                 - `class_preds`: The predicted class scores for each anchor.\n",
    "        \"\"\"\n",
    "        # Step 1: Feature extraction from the backbone (e.g., ResNet50)\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Step 2: Apply Top-Down Pathway aggregation\n",
    "        top_down_features = self.top_down(features)\n",
    "        \n",
    "        # Step 3: Apply Bottom-Up Pathway aggregation\n",
    "        bottom_up_features = self.bottom_up(features)\n",
    "        \n",
    "        # Step 4: Apply Adaptive Feature Pooling to all aggregated feature maps\n",
    "        pooled_top_down = [self.adaptive_pool(f) for f in top_down_features]\n",
    "        pooled_bottom_up = [self.adaptive_pool(f) for f in bottom_up_features]\n",
    "        \n",
    "        # Step 5: Predict bounding box offsets (Box Branch)\n",
    "        box_preds = [self.box_branch(f) for f in pooled_top_down + pooled_bottom_up]\n",
    "        \n",
    "        # Step 6: Predict class scores (Class Branch)\n",
    "        class_preds = [self.class_branch(f) for f in pooled_top_down + pooled_bottom_up]\n",
    "        \n",
    "        return box_preds, class_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a4599-d135-4a63-ba4c-a1f24b2b49de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
