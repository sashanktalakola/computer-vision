{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127646b8-64a6-49f6-b115-d17b35fd1c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74299333-e2ef-4434-813f-9295565e4087",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size, shift_size=0, mlp_ratio=4.0, dropout=0.1):\n",
    "        super(SwinTransformerBlock, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Window-based Multi-head Self-Attention (W-MSA)\n",
    "        self.attn = WindowAttention(dim, num_heads, window_size, shift_size, dropout)\n",
    "\n",
    "        # Feed-forward Network (MLP)\n",
    "        self.ffn = MLP(dim, int(dim * mlp_ratio), dropout)\n",
    "\n",
    "        # Layer Norms\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply LayerNorm\n",
    "        res = x\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Window-based Attention\n",
    "        x = self.attn(x)\n",
    "\n",
    "        # Add and norm\n",
    "        x = res + x\n",
    "        res = x\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # MLP\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        # Add and norm\n",
    "        x = res + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972704d-467e-4f13-958c-d9d0c31b8aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37194920-163e-43be-891c-e9fb45086207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size, shift_size=0, dropout=0.1):\n",
    "        super(WindowAttention, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "        # Define the relative position bias (used to compute self-attention)\n",
    "        self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * window_size - 1) ** 2, num_heads))\n",
    "\n",
    "        # Initialize\n",
    "        nn.init.trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        H = self.num_heads\n",
    "        window_size = self.window_size\n",
    "        shift_size = self.shift_size\n",
    "        \n",
    "        # Create Q, K, V\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, H, C // H).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Relative position bias\n",
    "        relative_position_bias = self.relative_position_bias_table.view(\n",
    "            window_size, window_size, window_size, window_size, -1\n",
    "        ).reshape(-1, H)\n",
    "        attention_map = torch.matmul(q, k.transpose(-2, -1)) + relative_position_bias\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_map = F.softmax(attention_map, dim=-1)\n",
    "        \n",
    "        # Dropout\n",
    "        attention_map = self.attn_drop(attention_map)\n",
    "\n",
    "        # Apply attention to values (v)\n",
    "        out = torch.matmul(attention_map, v)\n",
    "\n",
    "        # Project the output back to original dimension\n",
    "        out = out.permute(0, 2, 1, 3).reshape(B, N, C)\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb7bcc-e1fa-4d36-81d4-14381d470eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e77c64-68d0-4c47-b05c-9dd2a61b9fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, dropout=0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.relu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, in_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Patch Merging Layer\n",
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, dim, out_dim, stride=2):\n",
    "        super(PatchMerging, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.out_dim = out_dim\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.merge = nn.Linear(4 * dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        x = x.view(B, H, W, 4, C).permute(0, 3, 1, 2, 4).reshape(B, -1, 4 * C)\n",
    "        return self.merge(x)\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=4, in_channels=3, embed_dim=96):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.flatten = nn.Flatten(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf962b1-53f7-4805-a97d-315ccdd343b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3150648-3c7d-4224-adc0-34db2885eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=4, in_channels=3, embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24], window_size=7, mlp_ratio=4.0, num_classes=1000):\n",
    "        super(SwinTransformer, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.patch_embedding = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        num_layers = len(depths)\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(\n",
    "                nn.ModuleList([\n",
    "                    SwinTransformerBlock(\n",
    "                        dim=embed_dim * (2**i), \n",
    "                        num_heads=num_heads[i], \n",
    "                        window_size=window_size, \n",
    "                        shift_size=window_size//2, \n",
    "                        mlp_ratio=mlp_ratio\n",
    "                    ) for _ in range(depths[i])\n",
    "                ])\n",
    "            )\n",
    "            \n",
    "            if i < num_layers - 1:\n",
    "                self.layers.append(PatchMerging(embed_dim * (2**i), embed_dim * (2**(i+1))))\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim * (2**(num_layers-1)))\n",
    "\n",
    "        # Final classification head\n",
    "        self.head = nn.Linear(embed_dim * (2**(num_layers-1)), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            for block in layer:\n",
    "                x = block(x)\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = self.layers[i+1](x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90bbf61-644c-4c99-ba6e-ab1acdae4d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
