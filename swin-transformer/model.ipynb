{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127646b8-64a6-49f6-b115-d17b35fd1c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74299333-e2ef-4434-813f-9295565e4087",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size, shift_size=0, mlp_ratio=4.0, dropout=0.1):\n",
    "        super(SwinTransformerBlock, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Window-based Multi-head Self-Attention (W-MSA)\n",
    "        self.attn = WindowAttention(dim, num_heads, window_size, shift_size, dropout)\n",
    "\n",
    "        # Feed-forward Network (MLP)\n",
    "        self.ffn = MLP(dim, int(dim * mlp_ratio), dropout)\n",
    "\n",
    "        # Layer Norms\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply LayerNorm\n",
    "        res = x\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Window-based Attention\n",
    "        x = self.attn(x)\n",
    "\n",
    "        # Add and norm\n",
    "        x = res + x\n",
    "        res = x\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # MLP\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        # Add and norm\n",
    "        x = res + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972704d-467e-4f13-958c-d9d0c31b8aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37194920-163e-43be-891c-e9fb45086207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size, shift_size=0, dropout=0.1):\n",
    "        super(WindowAttention, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "        # Define the relative position bias (used to compute self-attention)\n",
    "        self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * window_size - 1) ** 2, num_heads))\n",
    "\n",
    "        # Initialize\n",
    "        nn.init.trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        H = self.num_heads\n",
    "        window_size = self.window_size\n",
    "        shift_size = self.shift_size\n",
    "        \n",
    "        # Create Q, K, V\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, H, C // H).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Relative position bias\n",
    "        relative_position_bias = self.relative_position_bias_table.view(\n",
    "            window_size, window_size, window_size, window_size, -1\n",
    "        ).reshape(-1, H)\n",
    "        attention_map = torch.matmul(q, k.transpose(-2, -1)) + relative_position_bias\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_map = F.softmax(attention_map, dim=-1)\n",
    "        \n",
    "        # Dropout\n",
    "        attention_map = self.attn_drop(attention_map)\n",
    "\n",
    "        # Apply attention to values (v)\n",
    "        out = torch.matmul(attention_map, v)\n",
    "\n",
    "        # Project the output back to original dimension\n",
    "        out = out.permute(0, 2, 1, 3).reshape(B, N, C)\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb7bcc-e1fa-4d36-81d4-14381d470eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
